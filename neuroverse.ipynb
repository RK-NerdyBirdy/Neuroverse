{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T21:37:57.556582Z",
     "iopub.status.busy": "2025-02-06T21:37:57.556361Z",
     "iopub.status.idle": "2025-02-06T21:37:58.195586Z",
     "shell.execute_reply": "2025-02-06T21:37:58.194441Z",
     "shell.execute_reply.started": "2025-02-06T21:37:57.556557Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in successfully!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login using your Hugging Face account token\n",
    "login(token=\"YOUR_HF_TOKEN\")\n",
    "print(\"Logged in successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T21:37:58.196520Z",
     "iopub.status.busy": "2025-02-06T21:37:58.196303Z",
     "iopub.status.idle": "2025-02-06T21:38:02.924602Z",
     "shell.execute_reply": "2025-02-06T21:38:02.923528Z",
     "shell.execute_reply.started": "2025-02-06T21:37:58.196498Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (2.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T21:38:02.925905Z",
     "iopub.status.busy": "2025-02-06T21:38:02.925672Z",
     "iopub.status.idle": "2025-02-06T21:41:36.250549Z",
     "shell.execute_reply": "2025-02-06T21:41:36.249042Z",
     "shell.execute_reply.started": "2025-02-06T21:38:02.925880Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n",
      "WARNING: Logging before InitGoogle() is written to STDERR\n",
      "E0000 00:00:1738877927.220128      10 common_lib.cc:612] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n",
      "=== Source Location Trace: === \n",
      "learning/45eac/tfrc/runtime/common_lib.cc:230\n",
      "Downloading shards: 100%|██████████| 2/2 [02:32<00:00, 76.21s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# ✅ Enable CUDA for faster loading\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load LLaMA tokenizer and model\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "llama_model.to(device)\n",
    "\n",
    "# Set padding token\n",
    "if llama_tokenizer.pad_token is None:\n",
    "    llama_tokenizer.pad_token = llama_tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:47:03.091170Z",
     "iopub.status.busy": "2025-02-06T22:47:03.090695Z",
     "iopub.status.idle": "2025-02-06T22:47:03.096766Z",
     "shell.execute_reply": "2025-02-06T22:47:03.096076Z",
     "shell.execute_reply.started": "2025-02-06T22:47:03.091136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_response(user_question, sentiment,frnd,expl):\n",
    "    #input_text = f\"User name: {frnd} User description: {expl}User prompt: {user_question} . feeling detected {sentiment}. Respond in a supportive, brief, and conversational tone. Offer helpful advice or jokes where appropriate, without a formal therapist-like approach. Focus on uplifting the user with empathy and positive suggestions, keeping the response clear and concise. Do not assume user input beyond the indicated sentiment. The response should feel like a friendly chatbot conversation. donot give additional notes or alternative responses. word limit 300 words. give your response after RESPONSE: keyword only\"\n",
    "    input_text= f\"\"\"You are a friendly AI counselor. Respond in a warm, supportive, and conversational tone. Do not assume beyond the given input or create user prompts. No extra notes or alternatives. Keep it ≤300 words. Reply like a chatbot, not a therapist. Respond only after \"RESPONSE:\" keyword if necessary.  \n",
    "analyse the prompt and give suggestions how to overcome that problem. keep your response in a plain text paragraph. \n",
    "User: {frnd} | Info: {expl} | Prompt: {prpt} | Emotion: {emotion} ({confidence}) .. \"\"\" \n",
    "\n",
    "    input_ids = llama_tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "    output =llama_model.generate(\n",
    "        input_ids, \n",
    "        max_length=400,  # ✅ Increase max tokens\n",
    "        num_return_sequences=1,  \n",
    "        temperature=0.7,  # ✅ More creative output\n",
    "        top_k=50,  # ✅ Sample diverse words\n",
    "        top_p=0.9,  \n",
    "        do_sample=True,  # ✅ Prevents deterministic outputs\n",
    "        pad_token_id=llama_tokenizer.eos_token_id  # ✅ Avoids abrupt cutoff\n",
    "    )\n",
    "\n",
    "    return llama_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:21:08.884593Z",
     "iopub.status.busy": "2025-02-06T22:21:08.884283Z",
     "iopub.status.idle": "2025-02-06T22:21:09.085212Z",
     "shell.execute_reply": "2025-02-06T22:21:09.084106Z",
     "shell.execute_reply.started": "2025-02-06T22:21:08.884566Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Load the emotion classification pipeline\n",
    "emotion_model = pipeline(\"text-classification\", model=\"bhadresh-savani/bert-base-go-emotion\", return_all_scores=True, device=device)\n",
    "def get_prominent_emotion(text):\n",
    "    emotions = emotion_model(text)[0]  # Get scores for all emotions\n",
    "    top_emotion = max(emotions, key=lambda x: x['score'])  # Find the highest score\n",
    "    return top_emotion['label'], top_emotion['score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:47:11.786116Z",
     "iopub.status.busy": "2025-02-06T22:47:11.785808Z",
     "iopub.status.idle": "2025-02-06T22:47:11.791560Z",
     "shell.execute_reply": "2025-02-06T22:47:11.790488Z",
     "shell.execute_reply.started": "2025-02-06T22:47:11.786089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate(user_question,frnd,expl):\n",
    "    # Get the top emotion\n",
    "    emotion, confidence = get_prominent_emotion(user_question)\n",
    "    print(f\"Emotion: {emotion} (Confidence: {confidence:.2f})\")\n",
    "    response = generate_response(user_question, f\"Emotion: {emotion} (Confidence: {confidence:.2f})\",frnd,expl)\n",
    "    try:\n",
    "        response_text = response.split(\"RESPONSE:\")[2].strip()\n",
    "        l = response_text.split(\"\\n\")\n",
    "        l = '\\n'.join(l[2:])\n",
    "        print(f\"Dost: {response_text}\")\n",
    "    except:\n",
    "        \n",
    "        response_text = response.split(\"..\").strip()\n",
    "        print(f\"Dost: {response_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T22:31:24.451927Z",
     "iopub.status.busy": "2025-02-06T22:31:24.451584Z",
     "iopub.status.idle": "2025-02-06T22:33:34.502428Z",
     "shell.execute_reply": "2025-02-06T22:33:34.501354Z",
     "shell.execute_reply.started": "2025-02-06T22:31:24.451897Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to your own A.I. counsellor. type:leave to exit the program\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is your name? : Maneet\n",
      "Explain yourself breifly College Student\n",
      "You:  I feel very Bad today ;-;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: sadness (Confidence: 0.34)\n",
      "Dost: I can tell you're having a tough day, Maneet. It sounds like sadness is really weighing on you. Would you like to talk about what's on your mind and why you're feeling this way? Sometimes sharing what's bothering you can help you feel a bit better. Is there something specific that's going on or has been going on that's causing you distress?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  my friends bully me too much these days. i feel insecure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: sadness (Confidence: 0.27)\n",
      "Dost: Sorry to hear that Maneet. It can be really tough to deal with bullying, especially when it's from people you consider friends. Feeling insecure can be a really challenging emotion to navigate. Here are some suggestions that might help: Have you talked to your friends about how their behavior is making you feel? Sometimes, people don't realize the impact of their words or actions, and talking it out can help clear the air. You might also consider talking to a trusted adult, like a teacher, counselor, or family member, about what's going on. They can offer you support and help you figure out ways to address the situation. Additionally, it might be helpful to surround yourself with people who support and uplift you. Joining a club or group that aligns with your interests can be a great way to meet new people who share your\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  leave\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to your own A.I. counsellor. type:leave to exit the program\")\n",
    "frnd = input(\"What is your name? :\")\n",
    "expl = input(\"Explain yourself breifly\")\n",
    "while(True):\n",
    "    prpt = input(\"You: \")\n",
    "    if prpt == \"leave\":break;\n",
    "    generate(prpt,frnd,expl)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-06T23:47:33.414Z",
     "iopub.execute_input": "2025-02-06T22:52:46.806096Z",
     "iopub.status.busy": "2025-02-06T22:52:46.805736Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to your own A.I. counsellor. type:leave to exit the program\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is your name? : Rohit\n",
      "Explain yourself breifly A war Veteran\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  I lost my limb in pakistani war. i still have nightmares of that day.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emotion: sadness (Confidence: 0.46)\n",
      "Dost: \" keyword, which was not present in the original message. I'll make sure to follow the correct protocol in the future. I'll assume that I should only respond when the \"\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to your own A.I. counsellor. type:leave to exit the program\")\n",
    "frnd = input(\"What is your name? :\")\n",
    "expl = input(\"Explain yourself breifly\")\n",
    "print()\n",
    "while(True):\n",
    "    prpt = input(\"You: \")\n",
    "    if prpt == \"leave\":break;\n",
    "    print()\n",
    "    generate(prpt,frnd,expl)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 236770,
     "modelInstanceId": 215076,
     "sourceId": 251597,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 236933,
     "modelInstanceId": 215238,
     "sourceId": 251783,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 237069,
     "modelInstanceId": 215368,
     "sourceId": 251919,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
